{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b16be1c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CD5C5C; padding: 20px; border-radius: 10px; box-shadow: 5px 5px 10px #888888;\">\n",
    "  <h1 style=\"color: white; font-size: 30px; font-weight: bold; text-align: center; text-shadow: 2px 2px 4px #000000;\">Sports Data Analysis: Problem Statement 3\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eecff21",
   "metadata": {},
   "source": [
    "## Data Source  \n",
    "\n",
    "The data source for this project is in the form of csv and understanding the nature, format is essential as it impacts the stages of ingestion and transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e13e78",
   "metadata": {},
   "source": [
    "## 1. Connecting to MySQL Database  \n",
    "\n",
    "Establishing a communication channel between our Python program and the database management system(MySQL). It is a crucial step in the data ingestion process, bringing data from the database into the Python environment for further analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2be7b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hithachoudhary\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\hithachoudhary\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mysql.connector.connection_cext.CMySQLConnection at 0x254397db880>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connecting to mysql workbench database\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import logging\n",
    "\n",
    "#Database connection parameters\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "   # 'database': 'sports_data',\n",
    "    'user': 'root',\n",
    "    'password': 'HithaChoudhary.21',\n",
    "    'port': '3307'\n",
    "}\n",
    "\n",
    "#Function to connect to the MySQL database\n",
    "def connect_to_database():\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        if connection.is_connected():\n",
    "            print(\"Connected to MySQL database\")\n",
    "            return connection\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to MySQL database: {e}\")\n",
    "\n",
    "connect_to_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7358caa",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion Strategy  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857d337",
   "metadata": {},
   "source": [
    "### 2.1 Setting up the table and database to further ingest the data from the csv file in a batch  \n",
    " \n",
    "-->The below code also includes logging to track the performance and reliability of the ingestion process. The 'data_ingestion.log' file is created to store the tracking process.  \n",
    "-->This stage also includes creation of index for improved query performance and faster data retrieval.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01892f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database and table setup complete.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "#Configuring logging\n",
    "logging.basicConfig(level=logging.INFO, filename='data_ingestion.log', filemode='a', format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'database': 'sports_data',\n",
    "    'password': 'HithaChoudhary.21',\n",
    "    'port': '3307'\n",
    "}\n",
    "\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "    \n",
    "cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS football_data (\n",
    "            ID INT PRIMARY KEY,\n",
    "            Unnamed INT,\n",
    "            Player VARCHAR(60),\n",
    "            Team VARCHAR(60),\n",
    "            Age INT,\n",
    "            Height FLOAT,\n",
    "            Weight FLOAT,\n",
    "            Position VARCHAR(50),\n",
    "            Goals INT,\n",
    "            Assists FLOAT,\n",
    "            YellowCards INT,\n",
    "            RedCards INT,\n",
    "            PassCompletionRate FLOAT,\n",
    "            DistanceCovered FLOAT,\n",
    "            Sprints INT,\n",
    "            ShotsOnTarget INT,\n",
    "            TacklesWon INT,\n",
    "            CleanSheets INT,\n",
    "            PlayerFatigue FLOAT,\n",
    "            MatchPressure INT,\n",
    "            InjuryHistory INT,\n",
    "            TrainingHours FLOAT,\n",
    "            FatigueInjuryCorrelation FLOAT,\n",
    "            PressurePerformanceImpact FLOAT,\n",
    "            EffectiveTraining FLOAT,\n",
    "            Season INT, \n",
    "            GoalsPerMatch FLOAT,\n",
    "            AssistsPerMatch FLOAT\n",
    "        )\n",
    "    ''')\n",
    "#Creation of index on the column positions for faster querying on that column.\n",
    "cursor.execute(\"SHOW INDEX FROM football_data WHERE Key_name = 'idx_position'\")\n",
    "index_exists = cursor.fetchone()\n",
    "if not index_exists:\n",
    "    cursor.execute('CREATE INDEX idx_position ON football_data (Position)')\n",
    "#Commiting the transaction\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"Database and table setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565eedd",
   "metadata": {},
   "source": [
    "## 3. Data Processing Plan  \n",
    "\n",
    "ETL(Extract,Transform,Load): This process includes extracting the data from the source, transforming the data with the help of python and loading it to the mysql databse server. The fusion of python and mysql helps complete the ETL process.\n",
    "\n",
    "-->The third and final process of ETL; loading of the data to the MySQL database is achievable with this code.  \n",
    "-->Incremental Loading is included in the code snippet below where it processess only new or modified data since the last successful data load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f5225a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_data(csv_file):\n",
    "    logging.info('Loading new data from CSV file.')\n",
    "    \n",
    "    #Loading the new data from the CSV file\n",
    "    new_data = pd.read_csv(csv_file)\n",
    "    conn = mysql.connector.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "    #Get existing indices from the database\n",
    "    cursor.execute(\"SELECT ID FROM football_data\")\n",
    "    existing_indexes = [row[0] for row in cursor.fetchall()]\n",
    "    #Filtering out records that already exist in the database\n",
    "    new_records = new_data[~new_data['ID'].isin(existing_indexes)]\n",
    "    logging.info(f'Found {len(new_records)} new records to insert.')\n",
    "    #Inserting new records into the database\n",
    "    for _, row in new_records.iterrows():\n",
    "        cursor.execute('''\n",
    "            INSERT INTO football_data (\n",
    "                'ID', 'Unnamed', 'Player', 'Team', 'Age', 'Height', 'Weight', 'Position',\n",
    "                'Goals', 'Assists', 'YellowCards', 'RedCards', 'PassCompletionRate',\n",
    "                'DistanceCovered', 'Sprints', 'ShotsOnTarget', 'TacklesWon',\n",
    "                'CleanSheets', 'PlayerFatigue', 'MatchPressure', 'InjuryHistory',\n",
    "                'TrainingHours', 'FatigueInjuryCorrelation', 'PressurePerformanceImpact',\n",
    "                'EffectiveTraining', 'Season', 'GoalsPerMatch', 'AssistsPerMatch'\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ''', tuple(row))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logging.info(f'{len(new_records)} new records inserted.')\n",
    "\n",
    "load_new_data('sports_dataset_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76350753",
   "metadata": {},
   "source": [
    "#### The above created design for a data ingestion pipeline is able to meet the requirements of:  \n",
    "(1)Connecting to the MySQL server database  \n",
    "(2)Ingesting the data using batch processing   \n",
    "(3)Creation of index to optimize storage  \n",
    "(4)Implementation of logging to monitor and track the ingestion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab09be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
